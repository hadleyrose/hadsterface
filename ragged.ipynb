{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ragged - Retrieval-Augmented Generation - Q&A with FAQs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is inspired by Huggingface's [RAG with source highlighting using Structured generation](https://huggingface.co/learn/cookbook/en/structured_generation) and [Advanced RAG on Hugging Face documentation using LangChain](https://huggingface.co/learn/cookbook/en/advanced_rag) demo notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Preface](#preface)\n",
    "1. [Setup](#setup)\n",
    "1. [Semantic Search BETWEEN Documents](#between)\n",
    "1. [LLM Q&A](#qa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preface <a name=\"preface\"></a>\n",
    "\n",
    "This is a Jupyter notebook. It is an interactive document with Markdown and code cells. The code cells can be run to output results directly in the notebook, as exemplified below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uncut gems\n"
     ]
    }
   ],
   "source": [
    "# I am a comment in a code cell\n",
    "my_variable = 'uncut gems'\n",
    "print(my_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, all notebook cells can be run **in any order**--re-running cells or running cells out of consecutive order can result in irreplicable or false results. Below, we will showcase running cells out of order and how that affects results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_variable = 'labyrinth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_string = 'I expect my string to mention the movie labyrinth'\n",
    "# should return True\n",
    "my_string == f'I expect my string to mention the movie {my_variable}'  # this is an f string, they're great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I expect my string to mention the movie labyrinth\n",
      "I expect my string to mention the movie labyrinth\n"
     ]
    }
   ],
   "source": [
    "print(my_string)\n",
    "print(f'I expect my string to mention the movie {my_variable}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the topmost code cell says `[5]` instead of `[1]`. We re-ran that code cell, updating `my_variable`'s assignment to `uncut gems`. Now, our results will not be consistent and will no longer read consecutively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I expect my string to mention the movie labyrinth\n",
      "I expect my string to mention the movie uncut gems\n"
     ]
    }
   ],
   "source": [
    "# should return False\n",
    "my_string == f'I expect my string to mention the movie {my_variable}'\n",
    "print(my_string)\n",
    "print(f'I expect my string to mention the movie {my_variable}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A notebook runs a Python environment via **a kernel**. We usually will see the name of the kernel in the top right corner of the notebook toolbar. The circle adjacent to the kernel name will be filled in when a cell is running or will indicate when there is an error connecting to the kernel.\n",
    "\n",
    "The kernel is just the connection between the notebook and the environment. The kernel is a great tool for configuring your notebook. The environment underlies the kernel; any package updates or installations are made directly to that environment.\n",
    "\n",
    "Our kernel is `.venv`. Below, using the `sys.executable` command, we are able to see which environment my kernel is using to run python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/G-DRIVE ArmorATD/hadsterface/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup <a name=\"setup\"></a>\n",
    "\n",
    "First, we will start by importing all relevant modules and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient  # LLM access and use\n",
    "import datasets  # access and load data\n",
    "from sentence_transformers import SentenceTransformer, util  # cosine similarity for semantic search\n",
    "import torch  # torch for conversion of data to tensors\n",
    "from getpass import getpass  # to safely input HF access token without sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load our data. Note that the following cell will **DOWNLOAD** the dataset if it isn't already available in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "doc_ds = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n",
    "type(doc_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our dataset is an `Arrow Dataset` type. It has two `features`, data inputs, and 2647 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'source'],\n",
       "    num_rows: 2647\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will convert the data to a pandas dataframe, which will convert each `feature` into a column. Each row will correspond to a data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Create an Endpoint\\n\\nAfter your first login,...</td>\n",
       "      <td>huggingface/hf-endpoints-documentation/blob/ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Choosing a metric for your task\\n\\n**So you'v...</td>\n",
       "      <td>huggingface/evaluate/blob/main/docs/source/cho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>主要特点\\n\\n让我们来介绍一下 Gradio 最受欢迎的一些功能！这里是 Gradio ...</td>\n",
       "      <td>gradio-app/gradio/blob/main/guides/cn/01_getti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>!--Copyright 2023 The HuggingFace Team. All ri...</td>\n",
       "      <td>huggingface/transformers/blob/main/docs/source...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gradio Demo: blocks_random_slider\\n\\n\\n```\\n!...</td>\n",
       "      <td>gradio-app/gradio/blob/main/demo/blocks_random...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0   Create an Endpoint\\n\\nAfter your first login,...   \n",
       "1   Choosing a metric for your task\\n\\n**So you'v...   \n",
       "2   主要特点\\n\\n让我们来介绍一下 Gradio 最受欢迎的一些功能！这里是 Gradio ...   \n",
       "3  !--Copyright 2023 The HuggingFace Team. All ri...   \n",
       "4   Gradio Demo: blocks_random_slider\\n\\n\\n```\\n!...   \n",
       "\n",
       "                                              source  \n",
       "0  huggingface/hf-endpoints-documentation/blob/ma...  \n",
       "1  huggingface/evaluate/blob/main/docs/source/cho...  \n",
       "2  gradio-app/gradio/blob/main/guides/cn/01_getti...  \n",
       "3  huggingface/transformers/blob/main/docs/source...  \n",
       "4  gradio-app/gradio/blob/main/demo/blocks_random...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_ds.set_format('pandas')\n",
    "doc_df = doc_ds[:]\n",
    "doc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the value of the `text` column for the first row, or data point, using pandas' subsetting features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Create an Endpoint\\n\\nAfter your first login, you will be directed to the [Endpoint creation page](https://ui.endpoints.huggingface.co/new). As an example, this guide will go through the steps to deploy [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) for text classification. \\n\\n## 1. Enter the Hugging Face Repository ID and your desired endpoint name:\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_repository.png\" alt=\"select repository\" />\\n\\n## 2. Select your Cloud Provider and region. Initially, only AWS will be available as a Cloud Provider with the `us-east-1` and `eu-west-1` regions. We will add Azure soon, and if you need to test Endpoints with other Cloud Providers or regions, please let us know.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_region.png\" alt=\"select region\" />\\n\\n## 3. Define the [Security Level](security) for the Endpoint:\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_security.png\" alt=\"define security\" />\\n\\n## 4. Create your Endpoint by clicking **Create Endpoint**. By default, your Endpoint is created with a medium CPU (2 x 4GB vCPUs with Intel Xeon Ice Lake) The cost estimate assumes the Endpoint will be up for an entire month, and does not take autoscaling into account.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_create_cost.png\" alt=\"create endpoint\" />\\n\\n## 5. Wait for the Endpoint to build, initialize and run which can take between 1 to 5 minutes.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/overview.png\" alt=\"overview\" />\\n\\n## 6. Test your Endpoint in the overview with the Inference widget 🏁 🎉!\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_inference.png\" alt=\"run inference\" />\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_df['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Search BETWEEN Documents <a name=\"between\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semantic Search** is the process of identifying similar results based on MEANING rather than keyword matches.\n",
    "\n",
    "A common method of semantic search is:\n",
    "1. Converting query text (your text, such as your search phrase) to its embedding (the mathematical representation of that text)\n",
    "2. Converting corpus text (the text you are comparing the query to) to its embeddings (this should use the same model as in step 1)\n",
    "3. Using a similarity function such as [cosine similarity](https://www.geeksforgeeks.org/cosine-similarity/) to return semantically similar results\n",
    "\n",
    "More on embeddings and cosine similarity can be found in these handy articles: [1](https://jalammar.github.io/illustrated-word2vec/) and [2](https://huggingface.co/tasks/sentence-similarity).\n",
    "\n",
    "Below, we will demonstrate semantic search on our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load a model to create embeddings. We will use [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2). We can review the [model card for all-MiniLM-L6-v2 on HuggingFace](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2). Model cards contain useful information such as how the model was trained, any biases the model may have, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/G-DRIVE ArmorATD/hadsterface/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will generate embeddings for every row in the `text` column of our dataset. We will do this using our model and its `encode` method.\n",
    "\n",
    "Then, we will convert each generated embedding from a numpy array to a tensor using `torch.from_numpy`. This is because our semantic search function requires tensor datatypes.\n",
    "\n",
    "Finally, we will use the `util.semantic_search` method to generate the top 10 similar `text` values for each `text` value in our dataset. Because we are comparing our dataset with itself, this is considered PAIRWISE semantic search. Consequently, we can expect our most similar text value to be the original query value compared to itself. We will demonstrate this below.\n",
    "\n",
    "**The following cell will take about 1 minute to run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>embedding</th>\n",
       "      <th>semantic_search</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Create an Endpoint\\n\\nAfter your first login,...</td>\n",
       "      <td>huggingface/hf-endpoints-documentation/blob/ma...</td>\n",
       "      <td>[tensor(0.0096), tensor(-0.0266), tensor(-0.00...</td>\n",
       "      <td>[{'corpus_id': 0, 'score': 0.9999998807907104}...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Choosing a metric for your task\\n\\n**So you'v...</td>\n",
       "      <td>huggingface/evaluate/blob/main/docs/source/cho...</td>\n",
       "      <td>[tensor(0.0092), tensor(-0.0732), tensor(-0.05...</td>\n",
       "      <td>[{'corpus_id': 1, 'score': 1.0000003576278687}...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>主要特点\\n\\n让我们来介绍一下 Gradio 最受欢迎的一些功能！这里是 Gradio ...</td>\n",
       "      <td>gradio-app/gradio/blob/main/guides/cn/01_getti...</td>\n",
       "      <td>[tensor(-0.0789), tensor(0.0448), tensor(-0.04...</td>\n",
       "      <td>[{'corpus_id': 2, 'score': 1.000000238418579},...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>!--Copyright 2023 The HuggingFace Team. All ri...</td>\n",
       "      <td>huggingface/transformers/blob/main/docs/source...</td>\n",
       "      <td>[tensor(-0.0926), tensor(0.0092), tensor(0.017...</td>\n",
       "      <td>[{'corpus_id': 3, 'score': 0.9999998807907104}...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gradio Demo: blocks_random_slider\\n\\n\\n```\\n!...</td>\n",
       "      <td>gradio-app/gradio/blob/main/demo/blocks_random...</td>\n",
       "      <td>[tensor(-0.0699), tensor(-0.0296), tensor(-0.0...</td>\n",
       "      <td>[{'corpus_id': 4, 'score': 1.0000003576278687}...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0   Create an Endpoint\\n\\nAfter your first login,...   \n",
       "1   Choosing a metric for your task\\n\\n**So you'v...   \n",
       "2   主要特点\\n\\n让我们来介绍一下 Gradio 最受欢迎的一些功能！这里是 Gradio ...   \n",
       "3  !--Copyright 2023 The HuggingFace Team. All ri...   \n",
       "4   Gradio Demo: blocks_random_slider\\n\\n\\n```\\n!...   \n",
       "\n",
       "                                              source  \\\n",
       "0  huggingface/hf-endpoints-documentation/blob/ma...   \n",
       "1  huggingface/evaluate/blob/main/docs/source/cho...   \n",
       "2  gradio-app/gradio/blob/main/guides/cn/01_getti...   \n",
       "3  huggingface/transformers/blob/main/docs/source...   \n",
       "4  gradio-app/gradio/blob/main/demo/blocks_random...   \n",
       "\n",
       "                                           embedding  \\\n",
       "0  [tensor(0.0096), tensor(-0.0266), tensor(-0.00...   \n",
       "1  [tensor(0.0092), tensor(-0.0732), tensor(-0.05...   \n",
       "2  [tensor(-0.0789), tensor(0.0448), tensor(-0.04...   \n",
       "3  [tensor(-0.0926), tensor(0.0092), tensor(0.017...   \n",
       "4  [tensor(-0.0699), tensor(-0.0296), tensor(-0.0...   \n",
       "\n",
       "                                     semantic_search  \n",
       "0  [{'corpus_id': 0, 'score': 0.9999998807907104}...  \n",
       "1  [{'corpus_id': 1, 'score': 1.0000003576278687}...  \n",
       "2  [{'corpus_id': 2, 'score': 1.000000238418579},...  \n",
       "3  [{'corpus_id': 3, 'score': 0.9999998807907104}...  \n",
       "4  [{'corpus_id': 4, 'score': 1.0000003576278687}...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate embeddings for each text data point\n",
    "doc_df['embedding'] = [model.encode(text) for text in doc_df['text'].to_list()]\n",
    "# convert embeddings from numpy arrays to tensors\n",
    "doc_df['embedding'] = [torch.from_numpy(embedding) for embedding in doc_df['embedding'].tolist()]\n",
    "query = doc_df['embedding'].to_list()\n",
    "corpus = doc_df['embedding'].to_list()\n",
    "# return top 10 most similar text values from dataset using cosine similarity\n",
    "doc_df['semantic_search'] = util.semantic_search(query, corpus)  # because our QUERY and our CORPUS is the same, this is considered pairwise semantic search\n",
    "doc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the semantic search results for the first row, we can see we have a list of ten dictionaries. The list is pre-sorted based on cosine similarity score, from highest to lowest. The closer to 1, the higher similarity. The closer to 0, the lower the similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'corpus_id': 1, 'score': 1.0000003576278687},\n",
       " {'corpus_id': 496, 'score': 0.678149938583374},\n",
       " {'corpus_id': 235, 'score': 0.6676350831985474},\n",
       " {'corpus_id': 833, 'score': 0.6462449431419373},\n",
       " {'corpus_id': 1585, 'score': 0.5992898941040039},\n",
       " {'corpus_id': 2542, 'score': 0.5690680742263794},\n",
       " {'corpus_id': 1665, 'score': 0.5668306946754456},\n",
       " {'corpus_id': 654, 'score': 0.5641615986824036},\n",
       " {'corpus_id': 1996, 'score': 0.549310028553009},\n",
       " {'corpus_id': 1024, 'score': 0.5416027903556824}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_df['semantic_search'].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we compared the same dataset to itself (pairwise semantic search), we can assume the first result for a given row, the one with highest similarity, will always be the given row itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'corpus_id': 1, 'score': 1.0000003576278687}\n"
     ]
    }
   ],
   "source": [
    "# we will assume the first item in pairwise semantic search will always be self since an item is expected to have highest similarity with itself\n",
    "print(doc_df['semantic_search'].iloc[1][0])  # should return corpus_id = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we returned the highest similarity results for the first row. The row with the highest similarity to row 1 is row 1 itself. If we want to find the highest similarity results that is not the original row itself, we need to return the SECOND result from semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'corpus_id': 496, 'score': 0.678149938583374}\n"
     ]
    }
   ],
   "source": [
    "# second item in list will be doc with highest similarity that isn't self\n",
    "print(doc_df['semantic_search'].iloc[1][1])  # should return any other corpus id but 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare row 1's `text` with row 496's `text` to see how similar they actually are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 1 text\n",
      " Choosing a metric for your task\n",
      "\n",
      "**So you've trained your model and want to see how well it’s doing\n",
      "\n",
      "\n",
      "Row 496 text\n",
      " A quick tour\n",
      "\n",
      "🤗 Evaluate provides access to a wide range of evaluation tools. It covers a range of \n"
     ]
    }
   ],
   "source": [
    "print('Row 1 text')\n",
    "print(doc_df['text'].iloc[1][:100])  # truncating for preview\n",
    "print('\\n')\n",
    "print('Row 496 text')\n",
    "print(doc_df['text'].iloc[496][:100])  # truncating for preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will return the most similar text that is not self for each row in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>embedding</th>\n",
       "      <th>semantic_search</th>\n",
       "      <th>most_similar_index</th>\n",
       "      <th>most_similar_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Create an Endpoint\\n\\nAfter your first login,...</td>\n",
       "      <td>huggingface/hf-endpoints-documentation/blob/ma...</td>\n",
       "      <td>[tensor(0.0096), tensor(-0.0266), tensor(-0.00...</td>\n",
       "      <td>[{'corpus_id': 0, 'score': 0.9999998807907104}...</td>\n",
       "      <td>983</td>\n",
       "      <td>Create a Private Endpoint with AWS PrivateLin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Choosing a metric for your task\\n\\n**So you'v...</td>\n",
       "      <td>huggingface/evaluate/blob/main/docs/source/cho...</td>\n",
       "      <td>[tensor(0.0092), tensor(-0.0732), tensor(-0.05...</td>\n",
       "      <td>[{'corpus_id': 1, 'score': 1.0000003576278687}...</td>\n",
       "      <td>496</td>\n",
       "      <td>A quick tour\\n\\n🤗 Evaluate provides access to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>主要特点\\n\\n让我们来介绍一下 Gradio 最受欢迎的一些功能！这里是 Gradio ...</td>\n",
       "      <td>gradio-app/gradio/blob/main/guides/cn/01_getti...</td>\n",
       "      <td>[tensor(-0.0789), tensor(0.0448), tensor(-0.04...</td>\n",
       "      <td>[{'corpus_id': 2, 'score': 1.000000238418579},...</td>\n",
       "      <td>2566</td>\n",
       "      <td>快速开始\\n\\n**先决条件**：Gradio 需要 Python 3.8 或更高版本，就...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>!--Copyright 2023 The HuggingFace Team. All ri...</td>\n",
       "      <td>huggingface/transformers/blob/main/docs/source...</td>\n",
       "      <td>[tensor(-0.0926), tensor(0.0092), tensor(0.017...</td>\n",
       "      <td>[{'corpus_id': 3, 'score': 0.9999998807907104}...</td>\n",
       "      <td>973</td>\n",
       "      <td>!--Copyright 2022 The HuggingFace Team. All ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gradio Demo: blocks_random_slider\\n\\n\\n```\\n!...</td>\n",
       "      <td>gradio-app/gradio/blob/main/demo/blocks_random...</td>\n",
       "      <td>[tensor(-0.0699), tensor(-0.0296), tensor(-0.0...</td>\n",
       "      <td>[{'corpus_id': 4, 'score': 1.0000003576278687}...</td>\n",
       "      <td>1917</td>\n",
       "      <td>Gradio Demo: interface_random_slider\\n\\n\\n```...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0   Create an Endpoint\\n\\nAfter your first login,...   \n",
       "1   Choosing a metric for your task\\n\\n**So you'v...   \n",
       "2   主要特点\\n\\n让我们来介绍一下 Gradio 最受欢迎的一些功能！这里是 Gradio ...   \n",
       "3  !--Copyright 2023 The HuggingFace Team. All ri...   \n",
       "4   Gradio Demo: blocks_random_slider\\n\\n\\n```\\n!...   \n",
       "\n",
       "                                              source  \\\n",
       "0  huggingface/hf-endpoints-documentation/blob/ma...   \n",
       "1  huggingface/evaluate/blob/main/docs/source/cho...   \n",
       "2  gradio-app/gradio/blob/main/guides/cn/01_getti...   \n",
       "3  huggingface/transformers/blob/main/docs/source...   \n",
       "4  gradio-app/gradio/blob/main/demo/blocks_random...   \n",
       "\n",
       "                                           embedding  \\\n",
       "0  [tensor(0.0096), tensor(-0.0266), tensor(-0.00...   \n",
       "1  [tensor(0.0092), tensor(-0.0732), tensor(-0.05...   \n",
       "2  [tensor(-0.0789), tensor(0.0448), tensor(-0.04...   \n",
       "3  [tensor(-0.0926), tensor(0.0092), tensor(0.017...   \n",
       "4  [tensor(-0.0699), tensor(-0.0296), tensor(-0.0...   \n",
       "\n",
       "                                     semantic_search  most_similar_index  \\\n",
       "0  [{'corpus_id': 0, 'score': 0.9999998807907104}...                 983   \n",
       "1  [{'corpus_id': 1, 'score': 1.0000003576278687}...                 496   \n",
       "2  [{'corpus_id': 2, 'score': 1.000000238418579},...                2566   \n",
       "3  [{'corpus_id': 3, 'score': 0.9999998807907104}...                 973   \n",
       "4  [{'corpus_id': 4, 'score': 1.0000003576278687}...                1917   \n",
       "\n",
       "                                   most_similar_text  \n",
       "0   Create a Private Endpoint with AWS PrivateLin...  \n",
       "1   A quick tour\\n\\n🤗 Evaluate provides access to...  \n",
       "2   快速开始\\n\\n**先决条件**：Gradio 需要 Python 3.8 或更高版本，就...  \n",
       "3  !--Copyright 2022 The HuggingFace Team. All ri...  \n",
       "4   Gradio Demo: interface_random_slider\\n\\n\\n```...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return the second-most similar (the non-self result) corpus id from semantic search for each row\n",
    "doc_df['most_similar_index'] = [result[1]['corpus_id'] for result in doc_df['semantic_search'].to_list()]\n",
    "# return the text value of each second-most similar result for each row\n",
    "doc_df['most_similar_text'] = [doc_df['text'].iloc[i] for i in doc_df['most_similar_index'].to_list()]\n",
    "doc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our results, we can see that in our dataset the text that is most similar to row 0 is row 983. Both of these texts discuss creating endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Create an Endpoint\\n\\nAfter your first login, you will be directed to the [Endpoint creation page](https://ui.endpoints.huggingface.co/new). As an example, this guide will go through the steps to deploy [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) for text classification. \\n\\n## 1. Enter the Hugging Face Repository ID and your desired endpoint name:\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_repository.png\" alt=\"select repository\" />\\n\\n## 2. Select your Cloud Provider and region. Initially, only AWS will be available as a Cloud Provider with the `us-east-1` and `eu-west-1` regions. We will add Azure soon, and if you need to test Endpoints with other Cloud Providers or regions, please let us know.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_region.png\" alt=\"select region\" />\\n\\n## 3. Define the [Security Level](security) for the Endpoint:\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_security.png\" alt=\"define security\" />\\n\\n## 4. Create your Endpoint by clicking **Create Endpoint**. By default, your Endpoint is created with a medium CPU (2 x 4GB vCPUs with Intel Xeon Ice Lake) The cost estimate assumes the Endpoint will be up for an entire month, and does not take autoscaling into account.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_create_cost.png\" alt=\"create endpoint\" />\\n\\n## 5. Wait for the Endpoint to build, initialize and run which can take between 1 to 5 minutes.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/overview.png\" alt=\"overview\" />\\n\\n## 6. Test your Endpoint in the overview with the Inference widget 🏁 🎉!\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_inference.png\" alt=\"run inference\" />\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_df.iloc[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Create a Private Endpoint with AWS PrivateLink\\n\\nSecurity and secure inference are key principles of Inference Endpoints. We currently offer three different levels of security: [Public, Protected and Private](/docs/inference-endpoints/security).\\n\\nPublic and Protected Endpoints do not require any additional configuration. But in order to create a Private Endpoint for a secure intra-region connection, you need to provide the AWS Account ID of the account which should also have access to Inference Endpoints.\\n\\n<img\\n  src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/6_private_type.png\"\\n  alt=\"select private link\"\\n/>\\n\\nAfter you provide your AWS Account ID and click **Create Endpoint**, the Inference Service creates a VPC Endpoint and you should see the VPC Service Name in your overview.\\n\\n<img\\n  src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/6_service_name.png\"\\n  alt=\"service link\"\\n/>\\n\\nThe VPC Service Name is used to create the VPC Interface Endpoint in your (customer) cloud account. Open your cloud account [console](https://console.aws.amazon.com/vpc/home?#Endpoints) to go create the VPC Interface Endpoint.\\n\\nAn example of the VPC Endpoint configuration is shown below. You will need to select the VPC and subnets, as well as the security groups you want to use.\\n\\n<img\\n  src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/6_aws.png\"\\n  alt=\"aws management console\"\\n/>\\n\\nOnce your Inference Endpoint is created successfully, go to the corresponding AWS account and add the VPC Endpoint as the endpoint.\\n\\n<img\\n  src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/6_private_with_url.png\"\\n  alt=\"endpoint url\"\\n/>\\n\\nAfter the VPC Endpoint status changes from **pending** to **available**, you should see a Endpoint URL in the overview. This URL can now be used inside your VPC to access your Endpoint in a secure and protected way, ensuring traffic is only occurring between the two endpoints and will never leave AWS.\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_df.iloc[0]['most_similar_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5928554534912109"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_df.iloc[0]['semantic_search'][1]['score']  # the cosine similarity score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, we will use this same semantic search approach as part of RAG Q&A, Retrieval-Augmented Generation Question & Answering. This will allow us to find the text that is most likely to contain the answer to our question. The model will then generate an answer based on that text.\n",
    "\n",
    "First, let's look at how we perform Q&A with an LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Q&A <a name=\"qa\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different ways to interact with LLMs. Because LLMs are large, we're going to use a method that doesn't download the model to our local machine. We will use a [huggingface access token](https://huggingface.co/docs/hub/security-tokens) to interact with the model. We will create our token as READ-ONLY."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Meta Llama 3 per the [inspiration tutorial here](https://huggingface.co/learn/cookbook/en/structured_generation). We can determine what prompt formats we can use for a model by reviewing its [model card on HuggingFace](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct). Looking at the [model card for Meta Llama 3](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct), we can identify it as a Text2Text Generation model that supports English and can be utilized in QA, Translation, and many other tasks.\n",
    "\n",
    "**NOTE THE WARNING** on Meta Llama 3's model card regarding data privacy and information sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I hope you're having a great day! I just wanted to check in and see how things are\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "my_token = getpass('HF access token:')\n",
    "\n",
    "llm_client = InferenceClient(model=repo_id, timeout=120, token=my_token)\n",
    "\n",
    "# Test your LLM client\n",
    "llm_client.text_generation(prompt=\"How are you today?\", max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we increase the `temperature` parameter, we'll get more \"creative\" or less accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Wishing you a double-tittled excellent day!\\n*- chord and*openedTam.’ thiOnly'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# play with temperature\n",
    "llm_client.text_generation(prompt='How are you today?', max_new_tokens=20, temperature=1.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the prompt from the [inspiration notebook](https://huggingface.co/learn/cookbook/en/structured_generation), we'll look at the different components of RAG Q&A.\n",
    "\n",
    "First, we have our **prompt**. This is the entire text sent to the LLM. It contains our instructions to the model on how we want our question answered. It also *can* contain our **context**, or the data we expect to find our answer within. Finally, it contains our **query**, or the question we want answered.\n",
    "\n",
    "Prompts can be formatted in many different ways. Prompt engineering is important to get the most out of our LLM.\n",
    "\n",
    "In the prompt below, we are instructing the model to use our `context` to answer the `user_query`. The values for `context` and `user_query` will be replaced with actual text using our code in later cells.\n",
    "\n",
    "Our prompt is also instructing the model to answer in JSON format. This is an example of using the generative capabalities of LLMs to not only return the answer to our question but to return it in a new format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://huggingface.co/learn/cookbook/en/structured_generation\n",
    "# define our prompt\n",
    "RAG_PROMPT_TEMPLATE_JSON = \"\"\"\n",
    "Answer the user query based on the source documents.\n",
    "\n",
    "Here are the source documents: {context}\n",
    "\n",
    "\n",
    "You should provide your answer as a JSON blob, and also provide all relevant short source snippets from the documents on which you directly based your answer, and a confidence score as a float between 0 and 1.\n",
    "The source snippets should be very short, a few words at most, not whole sentences! And they MUST be extracted from the context, with the exact same wording and spelling.\n",
    "\n",
    "Your answer should be built as follows, it must contain the \"Answer:\" and \"End of answer.\" sequences.\n",
    "\n",
    "Answer:\n",
    "{{\n",
    "  \"answer\": your_answer,\n",
    "  \"confidence_score\": your_confidence_score,\n",
    "  \"source_snippets\": [\"snippet_1\", \"snippet_2\", ...]\n",
    "}}\n",
    "End of answer.\n",
    "\n",
    "Now begin!\n",
    "Here is the user question: {user_query}.\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our question, same as before\n",
    "USER_QUERY = 'How are you?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document:\n",
      "\n",
      "I'm angry\n",
      "\n",
      "Document:\n",
      "\n",
      " Create an Endpoint\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define our data/text/corpus where an answer may be found\n",
    "RELEVANT_CONTEXT = f'''\n",
    "Document:\n",
    "\n",
    "I'm angry\n",
    "\n",
    "Document:\n",
    "\n",
    "{doc_df.iloc[0]['text'][:19]}\n",
    "'''  # in this f string, I am using a portion of one of our dataset's text values to provide another document as context\n",
    "\n",
    "print(RELEVANT_CONTEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer the user query based on the source documents.\n",
      "\n",
      "Here are the source documents: \n",
      "Document:\n",
      "\n",
      "I'm angry\n",
      "\n",
      "Document:\n",
      "\n",
      " Create an Endpoint\n",
      "\n",
      "\n",
      "\n",
      "You should provide your answer as a JSON blob, and also provide all relevant short source snippets from the documents on which you directly based your answer, and a confidence score as a float between 0 and 1.\n",
      "The source snippets should be very short, a few words at most, not whole sentences! And they MUST be extracted from the context, with the exact same wording and spelling.\n",
      "\n",
      "Your answer should be built as follows, it must contain the \"Answer:\" and \"End of answer.\" sequences.\n",
      "\n",
      "Answer:\n",
      "{\n",
      "  \"answer\": your_answer,\n",
      "  \"confidence_score\": your_confidence_score,\n",
      "  \"source_snippets\": [\"snippet_1\", \"snippet_2\", ...]\n",
      "}\n",
      "End of answer.\n",
      "\n",
      "Now begin!\n",
      "Here is the user question: How are you?.\n",
      "Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# insert our user-defined values into the prompt\n",
    "prompt = RAG_PROMPT_TEMPLATE_JSON.format(context=RELEVANT_CONTEXT, user_query=USER_QUERY)  # another string method, see f string method above\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"answer\": \"I\\'m angry\",\\n  \"confidence_score\": 0.8,\\n  \"source_snippets\": [\"I\\'m angry\"]\\n}\\nEnd of answer. \\n\\n\\n\\n\\n\\nPlease note that the confidence score is subjective and may vary based on the complexity of the query and the relevance of the source documents. In this case, the confidence score is 0.8 because the user query \"How are you?\" is somewhat related to the source document \"I\\'m angry\", but not very strongly. The confidence score can be adjusted based on the specific requirements of your application.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use our prompt and llm to generate an answer\n",
    "answer = llm_client.text_generation(\n",
    "    prompt,\n",
    "    max_new_tokens=1000\n",
    ")\n",
    "\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"answer\": \"I'm angry\",\n",
      "  \"confidence_score\": 0.8,\n",
      "  \"source_snippets\": [\"I'm angry\"]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split the model output so we only return the json object\n",
    "answer = answer.split(\"End of answer.\")[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Q&A <a name=\"rag\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, we manually defined our `context`. We did that by defining `RELEVANT_CONTEXT` with a static string value.\n",
    "\n",
    "Now, let's use our dataset to create the `context` using text rows most similar to our `query`, or question. We will do this by performing semantic search between our `query` and our dataset. The top 10 dataset rows similar to our `query` will be used to create a new `context`.\n",
    "\n",
    "We will place that `context` in our prompt and generate an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a new question, one we know our docs can answer\n",
    "USER_QUERY = 'how do create endpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'corpus_id': 1162, 'score': 0.5051618814468384},\n",
       "  {'corpus_id': 740, 'score': 0.44793304800987244},\n",
       "  {'corpus_id': 983, 'score': 0.441649466753006},\n",
       "  {'corpus_id': 1273, 'score': 0.4327011704444885},\n",
       "  {'corpus_id': 324, 'score': 0.4042762219905853},\n",
       "  {'corpus_id': 0, 'score': 0.3923899531364441},\n",
       "  {'corpus_id': 1147, 'score': 0.37026676535606384},\n",
       "  {'corpus_id': 1852, 'score': 0.36074841022491455},\n",
       "  {'corpus_id': 770, 'score': 0.3479536771774292},\n",
       "  {'corpus_id': 1278, 'score': 0.3412274420261383}]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate an embedding using the same model as we used on our dataset\n",
    "# then use cosine similarity to return the top 10 most similar data points from our dataset\n",
    "similar_docs_results = util.semantic_search(model.encode(USER_QUERY, convert_to_tensor=True), doc_df['embedding'].to_list())\n",
    "similar_docs_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Send Requests to Endpoints\\n\\nYou can send requests to Inference Endpoints using the UI leveraging the Inference Widget or programmatically, e.g. with cURL, `@huggingface/inference`, `huggingface_hub` or any REST client. The Endpoint overview not only provides a interactive widget for you to test the Endpoint, but also generates code for `python`, `javascript` and `curl`. You can use this code to quickly get started with your Endpoint in your favorite programming language.\\n\\nBelow are also examples on how to use the `@huggingface/inference` library to call an inference endpoint.\\n\\n## Use the UI to send requests\\n\\nThe Endpoint overview provides access to the Inference Widget which can be used to send requests (see step 6 of [Create an Endpoint](/docs/inference-endpoints/guides/create_endpoint)). This allows you to quickly test your Endpoint with different inputs and share it with team members.\\n\\n## Use cURL to send requests\\n\\nThe cURL command for the request above should look like this. You\\'ll need to provide your user token which can be found in your Hugging Face [account settings](https://huggingface.co/settings/tokens):\\n\\nExample Request:\\n\\n```bash\\ncurl https://uu149rez6gw9ehej.eu-west-1.aws.endpoints.huggingface.cloud/distilbert-sentiment \\\\\\n\\t-X POST \\\\\\n\\t-d \\'{\"inputs\": \"Deploying my first endpoint was an amazing experience.\"}\\' \\\\\\n\\t-H \"Authorization: Bearer <Token>\"\\n```\\n\\nThe Endpoints API offers the same API definitions as the [Inference API](https://huggingface.co/docs/api-inference/detailed_parameters) and the [SageMaker Inference Toolkit](https://huggingface.co/docs/sagemaker/reference#inference-toolkit-api). All the request payloads are documented in the [Supported Tasks](/docs/inference-endpoints/supported_tasks) section.\\n\\nThis means for an NLP task, the payload is represented as the `inputs` key and additional pipeline parameters are included in the `parameters` key. You can provide any of the supported `kwargs` from [pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines) as parameters.\\nFor image or audio tasks, you should send the data as a binary request with the corresponding mime type. Below is an example cURL for an audio payload:\\n\\n```bash\\ncurl --request POST \\\\\\n  --url https://uu149rez6gw9ehej.eu-west-1.aws.endpoints.huggingface.cloud/wav2vec-asr \\\\\\n  --header \\'Authorization: Bearer <Token>\\' \\\\\\n  --header \\'Content-Type: audio/x-flac\\' \\\\\\n  --data-binary \\'@sample1.flac\\'\\n```\\n\\nTo use your cURL command as code, use the [cURL Converter](https://curlconverter.com/) tool to quickly get started with the programming language of your choice.\\n\\n## Use javascript library `@huggingface/inference`\\n\\nYou can use the javascript library to call an inference endpoint:\\n\\n```ts\\nconst inference = new HfInference(\\'hf_...\\') // your user token\\n\\nconst gpt2 = inference.endpoint(\\'https://xyz.eu-west-1.aws.endpoints.huggingface.cloud/gpt2-endpoint\\')\\nconst { generated_text } = await gpt2.textGeneration({ inputs: \\'The answer to the universe is\\' })\\n```\\n\\n### Custom handler\\n\\n`@huggingface/inference` supports tasks from https://huggingface.co/tasks, and is typed accordingly.\\n\\nIf your model has additional inputs, or even custom inputs / outputs you can use the more generic `.request` / `streamingRequest`:\\n\\n```ts\\nconst output = await inference.request({\\n  inputs: \"blablabla\",\\n  parameters: {\\n    custom_parameter_1: ...,\\n    ...\\n  }\\n});\\n```\\n',\n",
       " \" Inference Endpoints\\n\\nInference Endpoints provides a secure production solution to easily deploy models on a dedicated and autoscaling infrastructure managed by Hugging Face. An Inference Endpoint is built from a model from the [Hub](https://huggingface.co/models). This page is a reference for `huggingface_hub`'s integration with Inference Endpoints. For more information about the Inference Endpoints product, check out its [official documentation](https://huggingface.co/docs/inference-endpoints/index).\\n\\n<Tip>\\n\\nCheck out the [related guide](../guides/inference_endpoints) to learn how to use `huggingface_hub` to manage your Inference Endpoints programmatically.\\n\\n</Tip>\\n\\nInference Endpoints can be fully managed via API. The endpoints are documented with [Swagger](https://api.endpoints.huggingface.cloud/). The [`InferenceEndpoint`] class is a simple wrapper built on top on this API.\\n\\n## Methods\\n\\nA subset of the Inference Endpoint features are implemented in [`HfApi`]:\\n\\n- [`get_inference_endpoint`] and [`list_inference_endpoints`] to get information about your Inference Endpoints\\n- [`create_inference_endpoint`], [`update_inference_endpoint`] and [`delete_inference_endpoint`] to deploy and manage Inference Endpoints\\n- [`pause_inference_endpoint`] and [`resume_inference_endpoint`] to pause and resume an Inference Endpoint\\n- [`scale_to_zero_inference_endpoint`] to manually scale an Endpoint to 0 replicas\\n\\n## InferenceEndpoint\\n\\nThe main dataclass is [`InferenceEndpoint`]. It contains information about a deployed `InferenceEndpoint`, including its configuration and current state. Once deployed, you can run inference on the Endpoint using the  [`InferenceEndpoint.client`] and [`InferenceEndpoint.async_client`] properties that respectively return an [`InferenceClient`] and an [`AsyncInferenceClient`] object.\\n\\n[[autodoc]] InferenceEndpoint\\n  - from_raw\\n  - client\\n  - async_client\\n  - all\\n\\n## InferenceEndpointStatus\\n\\n[[autodoc]] InferenceEndpointStatus\\n\\n## InferenceEndpointType\\n\\n[[autodoc]] InferenceEndpointType\\n\\n## InferenceEndpointError\\n\\n[[autodoc]] InferenceEndpointError\",\n",
       " ' Create a Private Endpoint with AWS PrivateLink\\n\\nSecurity and secure inference are key principles of Inference Endpoints. We currently offer three different levels of security: [Public, Protected and Private](/docs/inference-endpoints/security).\\n\\nPublic and Protected Endpoints do not require any additional configuration. But in order to create a Private Endpoint for a secure intra-region connection, you need to provide the AWS Account ID of the account which should also have access to Inference Endpoints.\\n\\n<img\\n  src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/6_private_type.png\"\\n  alt=\"select private link\"\\n/>\\n\\nAfter you provide your AWS Account ID and click **Create Endpoint**, the Inference Service creates a VPC Endpoint and you should see the VPC Service Name in your overview.\\n\\n<img\\n  src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/6_service_name.png\"\\n  alt=\"service link\"\\n/>\\n\\nThe VPC Service Name is used to create the VPC Interface Endpoint in your (customer) cloud account. Open your cloud account [console](https://console.aws.amazon.com/vpc/home?#Endpoints) to go create the VPC Interface Endpoint.\\n\\nAn example of the VPC Endpoint configuration is shown below. You will need to select the VPC and subnets, as well as the security groups you want to use.\\n\\n<img\\n  src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/6_aws.png\"\\n  alt=\"aws management console\"\\n/>\\n\\nOnce your Inference Endpoint is created successfully, go to the corresponding AWS account and add the VPC Endpoint as the endpoint.\\n\\n<img\\n  src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/6_private_with_url.png\"\\n  alt=\"endpoint url\"\\n/>\\n\\nAfter the VPC Endpoint status changes from **pending** to **available**, you should see a Endpoint URL in the overview. This URL can now be used inside your VPC to access your Endpoint in a secure and protected way, ensuring traffic is only occurring between the two endpoints and will never leave AWS.\\n',\n",
       " ' Change Organization or Account\\n\\nInference Endpoints uses your [Hugging Face](https://huggingface.co/) account - which can be either personal or an organization - to authenticate and deploy an Endpoint.\\nYou can switch between the two by clicking on the \"organization/user\" dropdown in the top right corner of the page. The dropdown will only show organizations and accounts that have an active plan. Check out the [Access the solution](/docs/inference-endpoints/guides/access) guide to learn more about setting up a plan.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/switch_organization_1.png\" alt=\"open organization select\" />\\n\\nSelect the organization you want to switch to. If you don\\'t have access to any organization, you can create a new one by clicking on the “Create Organization” button.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/switch_organization_2.png\" alt=\"organization select list\" />\\n',\n",
       " ' Access and view Metrics\\n\\nHugging Face Endpoints provides access to the metrics and analytics of your Endpoints through the UI on the detailed overview in the “Analytics” tab of your Endpoints. \\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/10_metric.png\" alt=\"metric dashboard\" />\\n\\n## Access Metrics via API\\n\\nThe Hugging Face Inference Endpoints API exposes a [route to access the metrics](/docs/inference-endpoints/api_reference#getendpointmetrics) of your Endpoints. You can use this route to send customized Prometheus queries to your Endpoints.',\n",
       " ' Create an Endpoint\\n\\nAfter your first login, you will be directed to the [Endpoint creation page](https://ui.endpoints.huggingface.co/new). As an example, this guide will go through the steps to deploy [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) for text classification. \\n\\n## 1. Enter the Hugging Face Repository ID and your desired endpoint name:\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_repository.png\" alt=\"select repository\" />\\n\\n## 2. Select your Cloud Provider and region. Initially, only AWS will be available as a Cloud Provider with the `us-east-1` and `eu-west-1` regions. We will add Azure soon, and if you need to test Endpoints with other Cloud Providers or regions, please let us know.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_region.png\" alt=\"select region\" />\\n\\n## 3. Define the [Security Level](security) for the Endpoint:\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_security.png\" alt=\"define security\" />\\n\\n## 4. Create your Endpoint by clicking **Create Endpoint**. By default, your Endpoint is created with a medium CPU (2 x 4GB vCPUs with Intel Xeon Ice Lake) The cost estimate assumes the Endpoint will be up for an entire month, and does not take autoscaling into account.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_create_cost.png\" alt=\"create endpoint\" />\\n\\n## 5. Wait for the Endpoint to build, initialize and run which can take between 1 to 5 minutes.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/overview.png\" alt=\"overview\" />\\n\\n## 6. Test your Endpoint in the overview with the Inference widget 🏁 🎉!\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_inference.png\" alt=\"run inference\" />\\n',\n",
       " ' Security & Compliance\\n\\n🤗 Inference Endpoints is built with security and secure inference at its core. Below you can find an overview of the security measures we have in place.\\n\\n## Data Security/Privacy\\n\\nHugging Face does not store any customer data in terms of payloads or tokens that are passed to the Inference Endpoint. We are storing logs for 30 days. Every Inference Endpoints uses TLS/SSL to encrypt the data in transit.\\n\\nWe also recommend using AWS or Azure Private Link for organizations. This allows you to access your Inference Endpoint through a private connection, without exposing it to the internet.\\n\\nHugging Face also offers Business Associate Addendum or GDPR data processing agreement through the Inference Endpoint enterprise plan.\\n\\n## Model Security/Privacy:\\n\\nYou can set a model repository as private if you do not want to publicly expose it. Hugging Face does not own any model or data you upload to the Hugging Face hub. Hugging Face does provide malware and pickle scans over the contents of the model repository as with all items in the Hub.\\n\\n## Inference Endpoints and Hub Security\\n\\nThe Hugging Face Hub, which Inference Endpoints is part, is also SOC2 Type 2 certified. The Hugging Face Hub offers Role Based Access Control. For more on hub security: https://huggingface.co/docs/hub/security\\n\\n<img width=\"150\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\">\\n\\n## Inference Endpoint Security level\\n\\nWe currently offer three types of endpoints, in order or increasing security level:\\n\\n- **Public**: A Public Endpoint is available from the internet, secured with TLS/SSL, and requires no authentication.\\n- **Protected**: A Protected Endpoint is available from the internet, secured with TLS/SSL, and requires a valid Hugging Face token for authentication.\\n- **Private** A Private Endpoint is only available through an intra-region secured AWS or Azure PrivateLink connection. Private Endpoints are not accessible from the internet.\\n\\nPublic and Protected Endpoints do not require any additional configuration. For Private Endpoints, you need to provide the AWS account ID of the account which also should have access to 🤗 Inference Endpoints.\\n\\n<img\\n  src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/endpoint_types.png\"\\n  alt=\"endpoint types\"\\n/>\\n\\nHugging Face Privacy Policy - https://huggingface.co/privacy\\n',\n",
       " ' Inference Endpoints\\n\\nInference Endpoints provides a secure production solution to easily deploy any `transformers`, `sentence-transformers`, and `diffusers` models on a dedicated and autoscaling infrastructure managed by Hugging Face. An Inference Endpoint is built from a model from the [Hub](https://huggingface.co/models). \\nIn this guide, we will learn how to programmatically manage Inference Endpoints with `huggingface_hub`. For more information about the Inference Endpoints product itself, check out its [official documentation](https://huggingface.co/docs/inference-endpoints/index).\\n\\nThis guide assumes `huggingface_hub` is correctly installed and that your machine is logged in. Check out the [Quick Start guide](https://huggingface.co/docs/huggingface_hub/quick-start#quickstart) if that\\'s not the case yet. The minimal version supporting Inference Endpoints API is `v0.19.0`.\\n\\n\\n## Create an Inference Endpoint\\n\\nThe first step is to create an Inference Endpoint using [`create_inference_endpoint`]:\\n\\n```py\\n>>> from huggingface_hub import create_inference_endpoint\\n\\n>>> endpoint = create_inference_endpoint(\\n...     \"my-endpoint-name\",\\n...     repository=\"gpt2\",\\n...     framework=\"pytorch\",\\n...     task=\"text-generation\",\\n...     accelerator=\"cpu\",\\n...     vendor=\"aws\",\\n...     region=\"us-east-1\",\\n...     type=\"protected\",\\n...     instance_size=\"medium\",\\n...     instance_type=\"c6i\"\\n... )\\n```\\n\\nIn this example, we created a `protected` Inference Endpoint named `\"my-endpoint-name\"`, to serve [gpt2](https://huggingface.co/gpt2) for `text-generation`. A `protected` Inference Endpoint means your token is required to access the API. We also need to provide additional information to configure the hardware requirements, such as vendor, region, accelerator, instance type, and size. You can check out the list of available resources [here](https://api.endpoints.huggingface.cloud/#/v2%3A%3Aprovider/list_vendors). Alternatively, you can create an Inference Endpoint manually using the [Web interface](https://ui.endpoints.huggingface.co/new) for convenience. Refer to this [guide](https://huggingface.co/docs/inference-endpoints/guides/advanced) for details on advanced settings and their usage.\\n\\nThe value returned by [`create_inference_endpoint`] is an [`InferenceEndpoint`] object:\\n\\n```py\\n>>> endpoint\\nInferenceEndpoint(name=\\'my-endpoint-name\\', namespace=\\'Wauplin\\', repository=\\'gpt2\\', status=\\'pending\\', url=None)\\n```\\n\\nIt\\'s a dataclass that holds information about the endpoint. You can access important attributes such as `name`, `repository`, `status`, `task`, `created_at`, `updated_at`, etc. If you need it, you can also access the raw response from the server with `endpoint.raw`.\\n\\nOnce your Inference Endpoint is created, you can find it on your [personal dashboard](https://ui.endpoints.huggingface.co/).\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/huggingface_hub/inference_endpoints_created.png)\\n\\n#### Using a custom image\\n\\nBy default the Inference Endpoint is built from a docker image provided by Hugging Face. However, it is possible to specify any docker image using the `custom_image` parameter. A common use case is to run LLMs using the [text-generation-inference](https://github.com/huggingface/text-generation-inference) framework. This can be done like this:\\n\\n```python\\n# Start an Inference Endpoint running Zephyr-7b-beta on TGI\\n>>> from huggingface_hub import create_inference_endpoint\\n>>> endpoint = create_inference_endpoint(\\n...     \"aws-zephyr-7b-beta-0486\",\\n...     repository=\"HuggingFaceH4/zephyr-7b-beta\",\\n...     framework=\"pytorch\",\\n...     task=\"text-generation\",\\n...     accelerator=\"gpu\",\\n...     vendor=\"aws\",\\n...     region=\"us-east-1\",\\n...     type=\"protected\",\\n...     instance_size=\"medium\",\\n...     instance_type=\"g5.2xlarge\",\\n...     custom_image={\\n...         \"health_route\": \"/health\",\\n...         \"env\": {\\n...             \"MAX_BATCH_PREFILL_TOKENS\": \"2048\",\\n...             \"MAX_INPUT_LENGTH\": \"1024\",\\n...             \"MAX_TOTAL_TOKENS\": \"1512\",\\n...             \"MODEL_ID\": \"/repository\"\\n...         },\\n...         \"url\": \"ghcr.io/huggingface/text-generation-inference:1.1.0\",\\n...     },\\n... )\\n```\\n\\nThe value to pass as `custom_image` is a dictionary containing a url to the docker container and configuration to run it. For more details about it, checkout the [Swagger documentation](https://api.endpoints.huggingface.cloud/#/v2%3A%3Aendpoint/create_endpoint).\\n\\n### Get or list existing Inference Endpoints\\n\\nIn some cases, you might need to manage Inference Endpoints you created previously. If you know the name, you can fetch it using [`get_inference_endpoint`], which returns an [`InferenceEndpoint`] object. Alternatively, you can use [`list_inference_endpoints`] to retrieve a list of all Inference Endpoints. Both methods accept an optional `namespace` parameter. You can set the `namespace` to any organization you are a part of. Otherwise, it defaults to your username.\\n\\n```py\\n>>> from huggingface_hub import get_inference_endpoint, list_inference_endpoints\\n\\n# Get one\\n>>> get_inference_endpoint(\"my-endpoint-name\")\\nInferenceEndpoint(name=\\'my-endpoint-name\\', namespace=\\'Wauplin\\', repository=\\'gpt2\\', status=\\'pending\\', url=None)\\n\\n# List all endpoints from an organization\\n>>> list_inference_endpoints(namespace=\"huggingface\")\\n[InferenceEndpoint(name=\\'aws-starchat-beta\\', namespace=\\'huggingface\\', repository=\\'HuggingFaceH4/starchat-beta\\', status=\\'paused\\', url=None), ...]\\n\\n# List all endpoints from all organizations the user belongs to\\n>>> list_inference_endpoints(namespace=\"*\")\\n[InferenceEndpoint(name=\\'aws-starchat-beta\\', namespace=\\'huggingface\\', repository=\\'HuggingFaceH4/starchat-beta\\', status=\\'paused\\', url=None), ...]\\n```\\n\\n## Check deployment status\\n\\nIn the rest of this guide, we will assume that we have a [`InferenceEndpoint`] object called `endpoint`. You might have noticed that the endpoint has a `status` attribute of type [`InferenceEndpointStatus`]. When the Inference Endpoint is deployed and accessible, the status should be `\"running\"` and the `url` attribute is set:\\n\\n```py\\n>>> endpoint\\nInferenceEndpoint(name=\\'my-endpoint-name\\', namespace=\\'Wauplin\\', repository=\\'gpt2\\', status=\\'running\\', url=\\'https://jpj7k2q4j805b727.us-east-1.aws.endpoints.huggingface.cloud\\')\\n```\\n\\nBefore reaching a `\"running\"` state, the Inference Endpoint typically goes through an `\"initializing\"` or `\"pending\"` phase. You can fetch the new state of the endpoint by running [`~InferenceEndpoint.fetch`]. Like every other method from [`InferenceEndpoint`] that makes a request to the server, the internal attributes of `endpoint` are mutated in place:\\n\\n```py\\n>>> endpoint.fetch()\\nInferenceEndpoint(name=\\'my-endpoint-name\\', namespace=\\'Wauplin\\', repository=\\'gpt2\\', status=\\'pending\\', url=None)\\n```\\n\\nInstead of fetching the Inference Endpoint status while waiting for it to run, you can directly call [`~InferenceEndpoint.wait`]. This helper takes as input a `timeout` and a `fetch_every` parameter (in seconds) and will block the thread until the Inference Endpoint is deployed. Default values are respectively `None` (no timeout) and `5` seconds.\\n\\n```py\\n# Pending endpoint\\n>>> endpoint\\nInferenceEndpoint(name=\\'my-endpoint-name\\', namespace=\\'Wauplin\\', repository=\\'gpt2\\', status=\\'pending\\', url=None)\\n\\n# Wait 10s => raises a InferenceEndpointTimeoutError\\n>>> endpoint.wait(timeout=10)\\n    raise InferenceEndpointTimeoutError(\"Timeout while waiting for Inference Endpoint to be deployed.\")\\nhuggingface_hub._inference_endpoints.InferenceEndpointTimeoutError: Timeout while waiting for Inference Endpoint to be deployed.\\n\\n# Wait more\\n>>> endpoint.wait()\\nInferenceEndpoint(name=\\'my-endpoint-name\\', namespace=\\'Wauplin\\', repository=\\'gpt2\\', status=\\'running\\', url=\\'https://jpj7k2q4j805b727.us-east-1.aws.endpoints.huggingface.cloud\\')\\n```\\n\\nIf `timeout` is set and the Inference Endpoint takes too much time to load, a [`InferenceEndpointTimeoutError`] timeout error is raised.\\n\\n## Run inference\\n\\nOnce your Inference Endpoint is up and running, you can finally run inference on it! \\n\\n[`InferenceEndpoint`] has two properties `client` and `async_client` returning respectively an [`InferenceClient`] and an [`AsyncInferenceClient`] objects.\\n\\n```py\\n# Run text_generation task:\\n>>> endpoint.client.text_generation(\"I am\")\\n\\' not a fan of the idea of a \"big-budget\" movie. I think it\\\\\\'s a\\'\\n\\n# Or in an asyncio context:\\n>>> await endpoint.async_client.text_generation(\"I am\")\\n```\\n\\nIf the Inference Endpoint is not running, an [`InferenceEndpointError`] exception is raised:\\n\\n```py\\n>>> endpoint.client\\nhuggingface_hub._inference_endpoints.InferenceEndpointError: Cannot create a client for this Inference Endpoint as it is not yet deployed. Please wait for the Inference Endpoint to be deployed using `endpoint.wait()` and try again.\\n```\\n\\nFor more details about how to use the [`InferenceClient`], check out the [Inference guide](../guides/inference).\\n\\n## Manage lifecycle\\n\\nNow that we saw how to create an Inference Endpoint and run inference on it, let\\'s see how to manage its lifecycle.\\n\\n<Tip>\\n\\nIn this section, we will see methods like [`~InferenceEndpoint.pause`], [`~InferenceEndpoint.resume`], [`~InferenceEndpoint.scale_to_zero`], [`~InferenceEndpoint.update`] and [`~InferenceEndpoint.delete`]. All of those methods are aliases added to [`InferenceEndpoint`] for convenience. If you prefer, you can also use the generic methods defined in `HfApi`: [`pause_inference_endpoint`], [`resume_inference_endpoint`], [`scale_to_zero_inference_endpoint`], [`update_inference_endpoint`], and [`delete_inference_endpoint`].\\n\\n</Tip>\\n\\n### Pause or scale to zero\\n\\nTo reduce costs when your Inference Endpoint is not in use, you can choose to either pause it using [`~InferenceEndpoint.pause`] or scale it to zero using [`~InferenceEndpoint.scale_to_zero`].\\n\\n<Tip>\\n\\nAn Inference Endpoint that is *paused* or *scaled to zero* doesn\\'t cost anything. The difference between those two is that a *paused* endpoint needs to be explicitly *resumed* using [`~InferenceEndpoint.resume`]. On the contrary, a *scaled to zero* endpoint will automatically start if an inference call is made to it, with an additional cold start delay. An Inference Endpoint can also be configured to scale to zero automatically after a certain period of inactivity.\\n\\n</Tip>\\n\\n```py\\n# Pause and resume endpoint\\n>>> endpoint.pause()\\nInferenceEndpoint(name=\\'my-endpoint-name\\', namespace=\\'Wauplin\\', repository=\\'gpt2\\', status=\\'paused\\', url=None)\\n>>> endpoint.resume()\\nInferenceEndpoint(name=\\'my-endpoint-name\\', namespace=\\'Wauplin\\', repository=\\'gpt2\\', status=\\'pending\\', url=None)\\n>>> endpoint.wait().client.text_generation(...)\\n...\\n\\n# Scale to zero\\n>>> endpoint.scale_to_zero()\\nInferenceEndpoint(name=\\'my-endpoint-name\\', namespace=\\'Wauplin\\', repository=\\'gpt2\\', status=\\'scaledToZero\\', url=\\'https://jpj7k2q4j805b727.us-east-1.aws.endpoints.huggingface.cloud\\')\\n# Endpoint is not \\'running\\' but still has a URL and will restart on first call.\\n```\\n\\n### Update model or hardware requirements\\n\\nIn some cases, you might also want to update your Inference Endpoint without creating a new one. You can either update the hosted model or the hardware requirements to run the model. You can do this using [`~InferenceEndpoint.update`]:\\n\\n```py\\n# Change target model\\n>>> endpoint.update(repository=\"gpt2-large\")\\nInferenceEndpoint(name=\\'my-endpoint-name\\', namespace=\\'Wauplin\\', repository=\\'gpt2-large\\', status=\\'pending\\', url=None)\\n\\n# Update number of replicas\\n>>> endpoint.update(min_replica=2, max_replica=6)\\nInferenceEndpoint(name=\\'my-endpoint-name\\', namespace=\\'Wauplin\\', repository=\\'gpt2-large\\', status=\\'pending\\', url=None)\\n\\n# Update to larger instance\\n>>> endpoint.update(accelerator=\"cpu\", instance_size=\"large\", instance_type=\"c6i\")\\nInferenceEndpoint(name=\\'my-endpoint-name\\', namespace=\\'Wauplin\\', repository=\\'gpt2-large\\', status=\\'pending\\', url=None)\\n```\\n\\n### Delete the endpoint\\n\\nFinally if you won\\'t use the Inference Endpoint anymore, you can simply call [`~InferenceEndpoint.delete()`].\\n\\n<Tip warning={true}>\\n\\nThis is a non-revertible action that will completely remove the endpoint, including its configuration, logs and usage metrics. You cannot restore a deleted Inference Endpoint.\\n\\n</Tip>\\n\\n\\n## An end-to-end example\\n\\nA typical use case of Inference Endpoints is to process a batch of jobs at once to limit the infrastructure costs. You can automate this process using what we saw in this guide:\\n\\n```py\\n>>> import asyncio\\n>>> from huggingface_hub import create_inference_endpoint\\n\\n# Start endpoint + wait until initialized\\n>>> endpoint = create_inference_endpoint(name=\"batch-endpoint\",...).wait()\\n\\n# Run inference\\n>>> client = endpoint.client\\n>>> results = [client.text_generation(...) for job in jobs]\\n\\n# Or with asyncio\\n>>> async_client = endpoint.async_client\\n>>> results = asyncio.gather(*[async_client.text_generation(...) for job in jobs])\\n\\n# Pause endpoint\\n>>> endpoint.pause()\\n```\\n\\nOr if your Inference Endpoint already exists and is paused:\\n\\n```py\\n>>> import asyncio\\n>>> from huggingface_hub import get_inference_endpoint\\n\\n# Get endpoint + wait until initialized\\n>>> endpoint = get_inference_endpoint(\"batch-endpoint\").resume().wait()\\n\\n# Run inference\\n>>> async_client = endpoint.async_client\\n>>> results = asyncio.gather(*[async_client.text_generation(...) for job in jobs])\\n\\n# Pause endpoint\\n>>> endpoint.pause()\\n```',\n",
       " ' Pause and Resume your Endpoint\\n\\nYou can `pause` & `resume` endpoints to save cost and configurations. Please note that if your endpoint is in a `failed` state, you will need to create a new endpoint. To `pause`/`resume` your endpoint, navigate to the \"overview\" tab and click the button at top right corner, which will show \"Pause endpoint\" to pause, or \"Resume endpoint\" to reactivate the paused endpoint.\\n\\nWhen pausing an endpoint the min & max number of replicas will be set to 0. When resuming an endpoint the min & max number of replicas will be set to 1. This allows you to programmatically pause and resume your endpoint by updating the \"min_replicas\" and \"max_replicas\" fields in the API.\\nPaused inference endpoints will have the following status: `PAUSED`. Paused endpoints will be NOT be billed until resumed. Pausing & Resuming an endpoint is a great way to save costs when you don\\'t need your endpoint to be running. For example, you can easily pause your endpoint during the night or weekends. You should pause your endpoint when you don\\'t need it for the time being.\\n\\nThe url of your endpoint will remain the same, even if you pause and resume it. This means that you can pause your endpoint and resume it later without having to update your code.\\n\\n## Pause an Inference Endpoint\\n\\nTo pause an endpoint, navigate to the \"overview\" tab and click the button at top right corner, which says \"Pause endpoint\".\\n\\n<img\\n  src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/pause_endpoint.png\"\\n  alt=\"Pause an Inference Endpoint\"\\n/>\\n\\nAfter clicking the button, you will be asked to confirm the action. Click \"Pause {ENDPOINT-NAME}\" to confirm.\\n\\n<img\\n  src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/pause_endpoint_confirm.png\"\\n  alt=\"Pause modal confirm Inference Endpoint\"\\n/>\\n\\nAfter that your replicas will be set to 0 and your endpoint will be paused. You can see the status change of your endpoint in the \"overview\" tab to `PAUSED`. If you do not see the `PAUSED` status make sure you\\'ve followed these instructions or contact us for help.\\n\\n<img\\n  src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/paused_endpoint.png\"\\n  alt=\"Paused Inference Endpoint\"\\n/>\\n\\n## Resume an Inference Endpoint\\n\\nTo resume an endpoint, navigate to the \"overview\" tab and click the button at top right corner showing \"Resume endpoint\".\\n\\n<img\\n  src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/resume_endpoint.png\"\\n  alt=\"Resume Inference Endpoint\"\\n/>\\n\\nYour endpoint will be resumed and the status will change to `Initalizing` and then to `Running`. Once your endpoint is running, you can start using it again and billing usage will incur.\\n',\n",
       " ' Update your Endpoint\\n\\nYou can update `running` Endpoints to change some of the configurations. However, if your endpoint is in a `failed` state, you need to create a new Endpoint. To update your endpoint you need to navigate to the \"settings\" tab.\\n\\nYou can update the instance type, autoscaling configuration, task and repository revision.\\n\\n## Instance size\\n\\nYou can update the instance size of your Endpoint in the Endpoint overview menu to match your evolving needs. For example, you can downsize to a smaller instance type if you don\\'t need the compute or alternatively, you can upgrade to a larger instance type if you need to increase your compute.\\n\\nYou\\'re able to update your _current_ instance type: CPU or GPU. There is no ability to update from one instance type to another (CPU to GPU or vice versa).\\n\\n<img\\n  src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/update_instance_type.png\"\\n  alt=\"Instance Type selection\"\\n/>\\n\\n## Autoscaling\\n\\nYou can update the autoscaling configuration of your Endpoint in the settings menu. Adjust the minimum and maximum number of replicas to upscale or downscale your Endpoint. Learn more about autoscaling [here](https://huggingface.co/docs/inference-endpoints/faq#q-how-does-autoscaling-work).\\n\\n## Task\\n\\nYou can update the task of your running Endpoint in the settings menu. The task defines the `pipeline` type your Endpoint will use and the inference widget on the Endpoint overview.\\n\\n## Revision\\n\\nYou can update the revision of your `running` Endpoint in the settings menu. The revision defines the version of the model repository you want to use for inference.\\n']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the corpus_ids from our semantic search results to get the most similar texts\n",
    "similar_docs = [doc_df['text'].iloc[result['corpus_id']] for result in similar_docs_results[0]]\n",
    "similar_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above results, we can see that the most similar texts to our query, 'how do create endpoint`, all contain the word endpoint.\n",
    "\n",
    "The most similar based on cosine similarity and the embeddings generated by our model was the data point containing the text 'Send Requests to Endpoints'.\n",
    "\n",
    "We can see that the text with the most appropriate answer, 'Create an Endpoint', is the 6th most similar result based on cosine similarity. Which model we used, which similarity function, the cleanliness of our data (see how our text still contains markdown elements?), whether or not we chunked our data, and more can affect our semantic search results. For now, we will use these results, but we can always refine our process based on these factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create the `context` for our prompt using the results from semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document:\n",
      " Send Requests to Endpoints\n",
      "\n",
      "You can send requests to Inference Endpoints using the UI leveraging th\n",
      "Document:\n",
      " Inference Endpoints\n",
      "\n",
      "Inference Endpoints provides a secure production solution to easily deploy mod\n",
      "Document:\n",
      " Create a Private Endpoint with AWS PrivateLink\n",
      "\n",
      "Security and secure inference are key principles of\n",
      "Document:\n",
      " Change Organization or Account\n",
      "\n",
      "Inference Endpoints uses your [Hugging Face](https://huggingface.co\n",
      "Document:\n",
      " Access and view Metrics\n",
      "\n",
      "Hugging Face Endpoints provides access to the metrics and analytics of you\n",
      "Document:\n",
      " Create an Endpoint\n",
      "\n",
      "After your first login, you will be directed to the [Endpoint creation page](ht\n",
      "Document:\n",
      " Security & Compliance\n",
      "\n",
      "🤗 Inference Endpoints is built with security and secure inference at its cor\n",
      "Document:\n",
      " Inference Endpoints\n",
      "\n",
      "Inference Endpoints provides a secure production solution to easily deploy any\n",
      "Document:\n",
      " Pause and Resume your Endpoint\n",
      "\n",
      "You can `pause` & `resume` endpoints to save cost and configuration\n",
      "Document:\n",
      " Update your Endpoint\n",
      "\n",
      "You can update `running` Endpoints to change some of the configurations. Howe\n"
     ]
    }
   ],
   "source": [
    "# truncate each text to only the first 100 characters due to model output limits\n",
    "# append 'Document:\\n' to each text in our semantic search results\n",
    "# then join all texts into a single string \n",
    "RELEVANT_CONTEXT = '\\n'.join(['Document:\\n' + text[:100] for text in similar_docs])\n",
    "\n",
    "print(RELEVANT_CONTEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer the user query based on the source documents.\n",
      "\n",
      "Here are the source documents: Document:\n",
      " Send Requests to Endpoints\n",
      "\n",
      "You can send requests to Inference Endpoints using the UI leveraging th\n",
      "Document:\n",
      " Inference Endpoints\n",
      "\n",
      "Inference Endpoints provides a secure production solution to easily deploy mod\n",
      "Document:\n",
      " Create a Private Endpoint with AWS PrivateLink\n",
      "\n",
      "Security and secure inference are key principles of\n",
      "Document:\n",
      " Change Organization or Account\n",
      "\n",
      "Inference Endpoints uses your [Hugging Face](https://huggingface.co\n",
      "Document:\n",
      " Access and view Metrics\n",
      "\n",
      "Hugging Face Endpoints provides access to the metrics and analytics of you\n",
      "Document:\n",
      " Create an Endpoint\n",
      "\n",
      "After your first login, you will be directed to the [Endpoint creation page](ht\n",
      "Document:\n",
      " Security & Compliance\n",
      "\n",
      "🤗 Inference Endpoints is built with security and secure inference at its cor\n",
      "Document:\n",
      " Inference Endpoints\n",
      "\n",
      "Inference Endpoints provides a secure production solution to easily deploy any\n",
      "Document:\n",
      " Pause and Resume your Endpoint\n",
      "\n",
      "You can `pause` & `resume` endpoints to save cost and configuration\n",
      "Document:\n",
      " Update your Endpoint\n",
      "\n",
      "You can update `running` Endpoints to change some of the configurations. Howe\n",
      "\n",
      "\n",
      "You should provide your answer as a JSON blob, and also provide all relevant short source snippets from the documents on which you directly based your answer, and a confidence score as a float between 0 and 1.\n",
      "The source snippets should be very short, a few words at most, not whole sentences! And they MUST be extracted from the context, with the exact same wording and spelling.\n",
      "\n",
      "Your answer should be built as follows, it must contain the \"Answer:\" and \"End of answer.\" sequences.\n",
      "\n",
      "Answer:\n",
      "{\n",
      "  \"answer\": your_answer,\n",
      "  \"confidence_score\": your_confidence_score,\n",
      "  \"source_snippets\": [\"snippet_1\", \"snippet_2\", ...]\n",
      "}\n",
      "End of answer.\n",
      "\n",
      "Now begin!\n",
      "Here is the user question: how do create endpoint.\n",
      "Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# insert our semantic search based context and user query into the prompt\n",
    "prompt = RAG_PROMPT_TEMPLATE_JSON.format(context=RELEVANT_CONTEXT, user_query=USER_QUERY)  # another string method, see f string method above\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"answer\": \"You can create an endpoint after your first login, you will be directed to the endpoint creation page.\",\\n  \"confidence_score\": 0.8,\\n  \"source_snippets\": [\"Create an Endpoint\", \"After your first login, you will be directed to the endpoint creation page\"]\\n}\\nEnd of answer.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate an answer\n",
    "answer = llm_client.text_generation(\n",
    "    prompt,\n",
    "    max_new_tokens=1000\n",
    ")\n",
    "\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"answer\": \"You can create an endpoint after your first login, you will be directed to the endpoint creation page.\",\n",
      "  \"confidence_score\": 0.8,\n",
      "  \"source_snippets\": [\"Create an Endpoint\", \"After your first login, you will be directed to the endpoint creation page\"]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer = answer.split(\"End of answer.\")[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can utilize different retrieval methods and different LLMs for this process.\n",
    "\n",
    "It is important to understand our LLMs configurations so we can adjust our prompts (format, length, etc.) to get the best results from the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<sub>made with ❤️ by [hadleyrose](https://github.com/hadleyrose)</sub>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
